[DEFAULT]
data path = /path to train-test folder/5-fold cross validation/
output path = /Path to output file/lr0_1/Results_DNN_DRD4_lr0_1_300epochs.csv
solver path = /Path to folder where solver file is stored/solver.prototxt
temp path = /Path to folder where Caffe snapshots are stored/DNN_Temp_DRD4/
name = DRD4
learning rate = 0.1
weight decay = 0.0005
epochs = 300
test interval = 300
number of classes = 2
batch size = 254
folds = 5


[Net1]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net2]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net3]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net4]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net5]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net6]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net7]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net8]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net9]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net10]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net11]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net12]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net13]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net14]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 5
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net15]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 10
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net16]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 50
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net17]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 100
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net18]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 200
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net19]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net20]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 700
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net21]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 1000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net22]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 1500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net23]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 2000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net24]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 2500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net25]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 3000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net26]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 2
#Number of neurons in each layer, separated by commas
network = 3500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net27]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 5
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net28]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 10
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net29]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 50
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net30]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 100
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net31]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 200
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net32]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net33]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 700
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net34]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 1000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net35]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 1500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net36]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 2000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net37]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 2500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net38]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 3000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net39]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 3
#Number of neurons in each layer, separated by commas
network = 3500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net40]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net41]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net42]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net43]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net44]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net45]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net46]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net47]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net48]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net48]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net49]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net50]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net51]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False


[Net52]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net53]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net54]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net55]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net56]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net57]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net58]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net59]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net60]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net61]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net62]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net63]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net64]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500
input dropout = 0
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net65]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net66]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net67]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net68]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net69]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net70]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net71]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net72]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net73]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net74]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net75]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net76]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net77]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net78]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net79]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net80]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net81]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net82]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net83]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net84]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net85]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net86]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net87]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net88]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net89]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net90]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net91]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5,5
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net92]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10,10
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net93]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50,50
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net94]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100,100
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net95]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200,200
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net96]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500,500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net97]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700,700
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net98]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000,1000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net99]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500,1500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net100]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000,2000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net101]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500,2500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net102]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000,3000
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net103]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500,3500
input dropout = 0
hidden dropout = 0
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net104]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5,5
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net105]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10,10
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net106]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50,50
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net107]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100,100
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net108]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200,200
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net109]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500,500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net110]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700,700
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net111]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000,1000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net112]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500,1500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net113]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000,2000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net114]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500,2500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net115]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000,3000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net116]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500,3500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False


[Net117]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5,5
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net118]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10,10
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net119]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50,50
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net120]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100,100
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net121]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200,200
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net122]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500,500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net123]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700,700
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net124]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000,1000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net125]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500,1500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net126]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000,2000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net127]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500,2500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net128]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000,3000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net129]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500,3500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False


[Net130]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5,5,5
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net131]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10,10,10
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net132]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50,50,50
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net133]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100,100
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net134]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200,200,200
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net135]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500,500,500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net136]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700,700,700
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net137]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000,1000,1000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net138]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500,1500,1500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net139]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000,2000,2000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net140]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500,2500,2500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net141]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000,3000,3000
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net142]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500,3500,3500
input dropout = 0.2
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net143]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 5,5,5,5
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net144]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 10,10,10,10
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net145]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 50,50,50,50
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net146]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 100,100,100
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net147]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 200,200,200,200
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net148]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 500,500,500,500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net149]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 700,700,700,700
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net150]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1000,1000,1000,1000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net151]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 1500,1500,1500,1500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net152]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2000,2000,2000,2000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net153]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 2500,2500,2500,2500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net154]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3000,3000,3000,3000
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False

[Net155]
#1 = ReLU, 2 = sigmoid, 3 = TanH
activation function = 1
#Number of neurons in each layer, separated by commas
network = 3500,3500,3500,3500
input dropout = 0.5
hidden dropout = 0.5
#1 = Xavier, 2 = Gaussian
filler = 2
l2 regularization = False
